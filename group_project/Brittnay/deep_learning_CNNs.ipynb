{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/ben: 30000 files\n",
      "train/mal: 30000 files\n",
      "valid/ben: 2000 files\n",
      "valid/mal: 2000 files\n",
      "test/ben: 4500 files\n",
      "test/mal: 4500 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For PyTorch or TensorFlow later\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DATA_DIR = Path(r\"C:\\Users\\heran\\Downloads\\total\")\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    for label in ['ben', 'mal']:\n",
    "        files = list((DATA_DIR / split / label).glob('*'))\n",
    "        print(f\"{split}/{label}: {len(files)} files\")\n",
    "\n",
    "MAX_LEN = 10000  # Number of bytes per file (pad/truncate to this length)\n",
    "\n",
    "def load_binary_file(path, max_len=MAX_LEN):\n",
    "    with open(path, 'rb') as f:\n",
    "        bytez = f.read()\n",
    "    byte_arr = np.frombuffer(bytez, dtype=np.uint8)\n",
    "    if len(byte_arr) > max_len:\n",
    "        byte_arr = byte_arr[:max_len]\n",
    "    else:\n",
    "        byte_arr = np.pad(byte_arr, (0, max_len - len(byte_arr)), 'constant')\n",
    "    return byte_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ben files from C:\\Users\\heran\\Downloads\\total\\train\\ben\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30000it [00:44, 677.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mal files from C:\\Users\\heran\\Downloads\\total\\train\\mal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30000it [00:44, 673.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ben files from C:\\Users\\heran\\Downloads\\total\\valid\\ben\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:02, 673.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mal files from C:\\Users\\heran\\Downloads\\total\\valid\\mal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:02, 788.88it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ben files from C:\\Users\\heran\\Downloads\\total\\test\\ben\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4500it [00:08, 544.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mal files from C:\\Users\\heran\\Downloads\\total\\test\\mal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4500it [00:07, 566.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (60000, 10000), Labels: [30000 30000]\n",
      "Val shape: (4000, 10000), Labels: [2000 2000]\n",
      "Test shape: (9000, 10000), Labels: [4500 4500]\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(split):\n",
    "    X, y = [], []\n",
    "    for label, folder in enumerate(['ben', 'mal']):\n",
    "        folder_path = DATA_DIR / split / folder\n",
    "        print(f'Loading {folder} files from {folder_path}')\n",
    "        for filepath in tqdm(folder_path.glob('*')):\n",
    "            if filepath.is_file():\n",
    "                X.append(load_binary_file(filepath))\n",
    "                y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load a small sample of training data to verify\n",
    "X_train, y_train = load_dataset('train')\n",
    "X_val, y_val = load_dataset('valid')\n",
    "X_test, y_test = load_dataset('test')\n",
    "\n",
    "print(f'Train shape: {X_train.shape}, Labels: {np.bincount(y_train)}')\n",
    "print(f'Val shape: {X_val.shape}, Labels: {np.bincount(y_val)}')\n",
    "print(f'Test shape: {X_test.shape}, Labels: {np.bincount(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape flat arrays (10000,) into (100, 100, 1)\n",
    "X_train_2d = X_train.reshape(-1, 100, 100, 1).astype('float32') / 255.0\n",
    "X_val_2d = X_val.reshape(-1, 100, 100, 1).astype('float32') / 255.0\n",
    "X_test_2d = X_test.reshape(-1, 100, 100, 1).astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to categorical \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_cat = to_categorical(y_train, 2)\n",
    "y_val_cat = to_categorical(y_val, 2)\n",
    "y_test_cat = to_categorical(y_test, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(100, 100, 1)),  # ðŸ‘ˆ this replaces input_shape in Conv2D\n",
    "\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(2, activation='softmax')  # 2 classes: benign, malicious\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7500/7500 - 503s - 67ms/step - accuracy: 0.7314 - loss: 0.5338 - val_accuracy: 0.7918 - val_loss: 0.4320\n",
      "Epoch 2/20\n",
      "7500/7500 - 493s - 66ms/step - accuracy: 0.7931 - loss: 0.4295 - val_accuracy: 0.8322 - val_loss: 0.3739\n",
      "Epoch 3/20\n",
      "7500/7500 - 492s - 66ms/step - accuracy: 0.8257 - loss: 0.3801 - val_accuracy: 0.8325 - val_loss: 0.3619\n",
      "Epoch 4/20\n",
      "7500/7500 - 491s - 66ms/step - accuracy: 0.8451 - loss: 0.3462 - val_accuracy: 0.8575 - val_loss: 0.3277\n",
      "Epoch 5/20\n",
      "7500/7500 - 494s - 66ms/step - accuracy: 0.8579 - loss: 0.3192 - val_accuracy: 0.8748 - val_loss: 0.3352\n",
      "Epoch 6/20\n",
      "7500/7500 - 512s - 68ms/step - accuracy: 0.8707 - loss: 0.2945 - val_accuracy: 0.8675 - val_loss: 0.3168\n",
      "Epoch 7/20\n",
      "7500/7500 - 556s - 74ms/step - accuracy: 0.8813 - loss: 0.2723 - val_accuracy: 0.8658 - val_loss: 0.3020\n",
      "Epoch 8/20\n",
      "7500/7500 - 582s - 78ms/step - accuracy: 0.8907 - loss: 0.2550 - val_accuracy: 0.8802 - val_loss: 0.2929\n",
      "Epoch 9/20\n",
      "7500/7500 - 572s - 76ms/step - accuracy: 0.8963 - loss: 0.2414 - val_accuracy: 0.8745 - val_loss: 0.2981\n",
      "Epoch 10/20\n",
      "7500/7500 - 568s - 76ms/step - accuracy: 0.9023 - loss: 0.2289 - val_accuracy: 0.8848 - val_loss: 0.2776\n",
      "Epoch 11/20\n",
      "7500/7500 - 576s - 77ms/step - accuracy: 0.9076 - loss: 0.2164 - val_accuracy: 0.8917 - val_loss: 0.3001\n",
      "Epoch 12/20\n",
      "7500/7500 - 588s - 78ms/step - accuracy: 0.9118 - loss: 0.2075 - val_accuracy: 0.8827 - val_loss: 0.2858\n",
      "Epoch 13/20\n",
      "7500/7500 - 496s - 66ms/step - accuracy: 0.9168 - loss: 0.1973 - val_accuracy: 0.8895 - val_loss: 0.2958\n",
      "Epoch 14/20\n",
      "7500/7500 - 500s - 67ms/step - accuracy: 0.9202 - loss: 0.1925 - val_accuracy: 0.8923 - val_loss: 0.2844\n",
      "Epoch 15/20\n",
      "7500/7500 - 695s - 93ms/step - accuracy: 0.9258 - loss: 0.1786 - val_accuracy: 0.8940 - val_loss: 0.2666\n",
      "Epoch 16/20\n",
      "7500/7500 - 620s - 83ms/step - accuracy: 0.9290 - loss: 0.1727 - val_accuracy: 0.9010 - val_loss: 0.2855\n",
      "Epoch 17/20\n",
      "7500/7500 - 517s - 69ms/step - accuracy: 0.9306 - loss: 0.1688 - val_accuracy: 0.8928 - val_loss: 0.2859\n",
      "Epoch 18/20\n",
      "7500/7500 - 542s - 72ms/step - accuracy: 0.9336 - loss: 0.1631 - val_accuracy: 0.8930 - val_loss: 0.3734\n",
      "Epoch 19/20\n",
      "7500/7500 - 510s - 68ms/step - accuracy: 0.9351 - loss: 0.1593 - val_accuracy: 0.9010 - val_loss: 0.3186\n",
      "Epoch 20/20\n",
      "7500/7500 - 555s - 74ms/step - accuracy: 0.9378 - loss: 0.1545 - val_accuracy: 0.8935 - val_loss: 0.3456\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_2d, y_train_cat,\n",
    "    validation_data=(X_val_2d, y_val_cat),\n",
    "    epochs=20,\n",
    "    batch_size=8,\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 89.44%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test_2d, y_test_cat, verbose=0)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
