{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/ben: 10922 files\n",
      "train/mal: 10922 files\n",
      "valid/ben: 2500 files\n",
      "valid/mal: 2446 files\n",
      "test/ben: 10000 files\n",
      "test/mal: 10000 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For PyTorch or TensorFlow later\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DATA_DIR = Path(r\"D:\\project\\myJupyter\\deep-learning-with-python-notebooks-master\\data_exe_100000\")\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    for label in ['ben', 'mal']:\n",
    "        files = list((DATA_DIR / split / label).glob('*'))\n",
    "        print(f\"{split}/{label}: {len(files)} files\")\n",
    "\n",
    "MAX_LEN = 10000  # Number of bytes per file (pad/truncate to this length)\n",
    "\n",
    "def load_binary_file(path, max_len=MAX_LEN):\n",
    "    with open(path, 'rb') as f:\n",
    "        bytez = f.read()\n",
    "    byte_arr = np.frombuffer(bytez, dtype=np.uint8)\n",
    "    if len(byte_arr) > max_len:\n",
    "        byte_arr = byte_arr[:max_len]\n",
    "    else:\n",
    "        byte_arr = np.pad(byte_arr, (0, max_len - len(byte_arr)), 'constant')\n",
    "    return byte_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ben files from D:\\project\\myJupyter\\deep-learning-with-python-notebooks-master\\data_exe_100000\\train\\ben\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10922it [02:07, 85.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mal files from D:\\project\\myJupyter\\deep-learning-with-python-notebooks-master\\data_exe_100000\\train\\mal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10922it [01:46, 102.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ben files from D:\\project\\myJupyter\\deep-learning-with-python-notebooks-master\\data_exe_100000\\valid\\ben\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2500it [00:16, 154.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mal files from D:\\project\\myJupyter\\deep-learning-with-python-notebooks-master\\data_exe_100000\\valid\\mal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:24, 101.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ben files from D:\\project\\myJupyter\\deep-learning-with-python-notebooks-master\\data_exe_100000\\test\\ben\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [01:25, 116.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mal files from D:\\project\\myJupyter\\deep-learning-with-python-notebooks-master\\data_exe_100000\\test\\mal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [01:50, 90.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (21844, 10000), Labels: [10922 10922]\n",
      "Val shape: (4946, 10000), Labels: [2500 2446]\n",
      "Test shape: (20000, 10000), Labels: [10000 10000]\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(split):\n",
    "    X, y = [], []\n",
    "    for label, folder in enumerate(['ben', 'mal']):\n",
    "        folder_path = DATA_DIR / split / folder\n",
    "        print(f'Loading {folder} files from {folder_path}')\n",
    "        for filepath in tqdm(folder_path.glob('*')):\n",
    "            if filepath.is_file():\n",
    "                X.append(load_binary_file(filepath))\n",
    "                y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load a small sample of training data to verify\n",
    "X_train, y_train = load_dataset('train')\n",
    "X_val, y_val = load_dataset('valid')\n",
    "X_test, y_test = load_dataset('test')\n",
    "\n",
    "print(f'Train shape: {X_train.shape}, Labels: {np.bincount(y_train)}')\n",
    "print(f'Val shape: {X_val.shape}, Labels: {np.bincount(y_val)}')\n",
    "print(f'Test shape: {X_test.shape}, Labels: {np.bincount(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape flat arrays (10000,) into (100, 100, 1)\n",
    "X_train_2d = X_train.reshape(-1, 100, 100, 1).astype('float32') / 255.0\n",
    "X_val_2d = X_val.reshape(-1, 100, 100, 1).astype('float32') / 255.0\n",
    "X_test_2d = X_test.reshape(-1, 100, 100, 1).astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to categorical \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_cat = to_categorical(y_train, 2)\n",
    "y_val_cat = to_categorical(y_val, 2)\n",
    "y_test_cat = to_categorical(y_test, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(100, 100, 1)),  # ðŸ‘ˆ this replaces input_shape in Conv2D\n",
    "\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(2, activation='softmax')  # 2 classes: benign, malicious\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2731/2731 - 153s - 56ms/step - accuracy: 0.6852 - loss: 0.6294 - val_accuracy: 0.7523 - val_loss: 0.4991\n",
      "Epoch 2/20\n",
      "2731/2731 - 149s - 55ms/step - accuracy: 0.7534 - loss: 0.4912 - val_accuracy: 0.7467 - val_loss: 0.4938\n",
      "Epoch 3/20\n",
      "2731/2731 - 149s - 55ms/step - accuracy: 0.7777 - loss: 0.4576 - val_accuracy: 0.7764 - val_loss: 0.4595\n",
      "Epoch 4/20\n",
      "2731/2731 - 157s - 57ms/step - accuracy: 0.8060 - loss: 0.4148 - val_accuracy: 0.8201 - val_loss: 0.3935\n",
      "Epoch 5/20\n",
      "2731/2731 - 157s - 57ms/step - accuracy: 0.8254 - loss: 0.3804 - val_accuracy: 0.8188 - val_loss: 0.3934\n",
      "Epoch 6/20\n",
      "2731/2731 - 155s - 57ms/step - accuracy: 0.8423 - loss: 0.3495 - val_accuracy: 0.7938 - val_loss: 0.4444\n",
      "Epoch 7/20\n",
      "2731/2731 - 154s - 56ms/step - accuracy: 0.8613 - loss: 0.3219 - val_accuracy: 0.8463 - val_loss: 0.3712\n",
      "Epoch 8/20\n",
      "2731/2731 - 153s - 56ms/step - accuracy: 0.8709 - loss: 0.2966 - val_accuracy: 0.8457 - val_loss: 0.3595\n",
      "Epoch 9/20\n",
      "2731/2731 - 158s - 58ms/step - accuracy: 0.8849 - loss: 0.2742 - val_accuracy: 0.8506 - val_loss: 0.3453\n",
      "Epoch 10/20\n",
      "2731/2731 - 156s - 57ms/step - accuracy: 0.8931 - loss: 0.2525 - val_accuracy: 0.8441 - val_loss: 0.3773\n",
      "Epoch 11/20\n",
      "2731/2731 - 156s - 57ms/step - accuracy: 0.9008 - loss: 0.2388 - val_accuracy: 0.8518 - val_loss: 0.3738\n",
      "Epoch 12/20\n",
      "2731/2731 - 153s - 56ms/step - accuracy: 0.9059 - loss: 0.2277 - val_accuracy: 0.8567 - val_loss: 0.3411\n",
      "Epoch 13/20\n",
      "2731/2731 - 152s - 56ms/step - accuracy: 0.9150 - loss: 0.2092 - val_accuracy: 0.8621 - val_loss: 0.3738\n",
      "Epoch 14/20\n",
      "2731/2731 - 157s - 58ms/step - accuracy: 0.9203 - loss: 0.1991 - val_accuracy: 0.8550 - val_loss: 0.4181\n",
      "Epoch 15/20\n",
      "2731/2731 - 159s - 58ms/step - accuracy: 0.9229 - loss: 0.1935 - val_accuracy: 0.8562 - val_loss: 0.3826\n",
      "Epoch 16/20\n",
      "2731/2731 - 153s - 56ms/step - accuracy: 0.9289 - loss: 0.1818 - val_accuracy: 0.8631 - val_loss: 0.3419\n",
      "Epoch 17/20\n",
      "2731/2731 - 153s - 56ms/step - accuracy: 0.9341 - loss: 0.1689 - val_accuracy: 0.8558 - val_loss: 0.3682\n",
      "Epoch 18/20\n",
      "2731/2731 - 154s - 56ms/step - accuracy: 0.9360 - loss: 0.1625 - val_accuracy: 0.8613 - val_loss: 0.4221\n",
      "Epoch 19/20\n",
      "2731/2731 - 153s - 56ms/step - accuracy: 0.9405 - loss: 0.1533 - val_accuracy: 0.8635 - val_loss: 0.4689\n",
      "Epoch 20/20\n",
      "2731/2731 - 153s - 56ms/step - accuracy: 0.9423 - loss: 0.1435 - val_accuracy: 0.8530 - val_loss: 0.5502\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_2d, y_train_cat,\n",
    "    validation_data=(X_val_2d, y_val_cat),\n",
    "    epochs=20,\n",
    "    batch_size=8,\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.85%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test_2d, y_test_cat, verbose=0)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
